<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Qitai Wang</title>
  
  <meta name="author" content="Qitai Wang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/icon.png">
</head>

<body>
  <table style="width:100%;max-width:850px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:60%;vertical-align:middle">
              <p style="text-align:center">
                <name>Qitai Wang</name>
              </p>
              <p>
              I am currently a fifth-year Ph.D. student at the NLPR, Institute of Automation, Chinese Academy of Sciences (CASIA), supervised by Prof. <a href="https://zhaoxiangzhang.net" target="_blank">Zhaoxiang Zhang</a>. Prior to that, I obtained my Bachelor's degree in Automation from the Department of Automation, Tsinghua University in 2020.
              Additionally, I have interned at <a href="https://www.tusimple.com/">TuSimple</a>.
              </p>  
              <p>
              My research interests involves <strong> computer vision</strong>, <strong> 3D perceptions</strong>, <strong>video generation models</strong> and <strong>driving simulation</strong>. I am currently exploring fully generative driving simulation.
              </p>
              <p style="text-align:center">
                <a href="mailto:wangqitai2020@ia.ac.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=RgM_VVIAAAAJ"> Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/esdolo"> Github </a> &nbsp/&nbsp
                <a href="data/CV_Yuqi_Wang.pdf">Curriculum Vitae</a>
              </p>
            </td>
            <td style="padding:2.5%;width:30%;max-width:30%">
              <img style="width:80%;max-width:80%" alt="profile photo" src="img/wqtandbird.jpg">
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
            <p>
              <li style="margin: 5px;" >
                <b>2024-09:</b> <strong>One</strong> paper on 3D perception is accepted to <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=34">TPAMI</a>.
              <li style="margin: 5px;" >
                <b>2025-01:</b> <strong>One</strong> paper on scene reconstruction-based driving simulation is accepted to <a href="http://cvpr2025.thecvf.com/">CVPR 2025</a>..
              <li style="margin: 5px;" >
                <b>2025-01:</b> <strong>One</strong> paper on generative driving simulation is accepted to <a href="https://iclr.cc/">ICLR 2025</a>.
              <li style="margin: 5px;" >
                <b>2024-09:</b> <strong>One</strong> paper on driving world model is accepted to <a href="https://neurips.cc/">NeurIPS 2024 Dataset Track</a>.
              <li style="margin: 5px;" >
                <b>2024-07:</b> <strong>One</strong> paper on End-to-End Multi-Object Tracking is accepted to <a href="https://eccv.ecva.net/">ECCV 2024</a>.
              </li>
            </p>
          </td>
        </tr>
      </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                * indicates equal contribution
              </p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody></tbody>

          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/freesim.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>FreeSim: Toward Free-viewpoint Camera Simulation in Driving Scenes</papertitle>
                <br>
                <a> Lue Fan</a>, <a>Hao Zhang</a>, <strong>Qitai Wang</strong>, <a>Hongsheng Li</a>, <a>Zhaoxiang Zhang</a>
                <br>
                <em>CVPR 2025  
                <br>
                <a href="https://arxiv.org/abs/2412.03566">[paper]</a> <a href="https://drive-sim.github.io/freesim">[Page]</a>
                <br>
                <p> After FreeVS we propose FreeSim, a generation-reconstruction hybrid method for free-viewpoint camera simulation, taking the best of two worlds! </p>
            </td>
            </tr>
          
          <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/freevs1.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>FreeVS: Generative View Synthesis on Free Driving Trajectory</papertitle>
                <br>
                <strong>Qitai Wang</strong>, <a>Lue Fan</a>, <a> Yuqi Wang</a>, <a>Yuntao Chen</a>, <a>Zhaoxiang Zhang</a>
                <br>
                <em>ICLR, 2025
                <br>
                <a href="https://arxiv.org/abs/2410.18079">[paper]</a> <a href="https://freevs24.github.io/">[Page]</a>  <a href="https://github.com/Robertwyq/Drivingdojo">[code]</a>
                <br>
                <p> FreeVS is the first method that supports high-quality generative view synthesis on free driving trajectory. A crucial step towards achieving generative driving simulation.  </p>
            </td>
            </tr>
          
        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/drivingdojo.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>DrivingDojo Dataset: Advancing Interactive and Knowledge-Enriched Driving World Model</papertitle>
                <br>
                <a> Yuqi Wang</a>*, <a>Ke Cheng</a>*, <a>Jiawei He</a>*, <strong>Qitai Wang</strong>*, <a>Hengchen Dai</a>, <a>Yuntao Chen</a>, <a>Fei Xia</a>, <a>Zhaoxiang Zhang</a>
                <br>
                <em>NeurIPS, 2024, D&B Track
                <br>
                <a href="https://arxiv.org/pdf/2410.10738v1">[paper]</a> <a href="https://drivingdojo.github.io/">[Page]</a> <a href="https://github.com/Robertwyq/Drivingdojo">[code]</a>
                <br>
                <p> DrivingDojo dataset  features video clips with a complete set of driving maneuvers, diverse multi-agent interplay, and rich open-world driving knowledge, laying a stepping stone for future world model development. </p>
            </td>
            </tr>

        <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/onetrack.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>OneTrack: Demystifying the Conflict Between Detection and Tracking in End-to-End 3D Trackers</papertitle>
                <br>
                <strong>Qitai Wang</strong>, <a>Jiawei He</a>, <a>Yuntao Chen</a>, <a>Zhaoxiang Zhang</a>
                <br>
                <em>ECCV, 2024
                <br>
                <a href="https://link.springer.com/chapter/10.1007/978-3-031-72667-5_22">[paper]</a>
                <br>
                <p> We have completely resolved the challenge where the perception performance of end-to-end multi-object tracking was inferior to that of standalone detectors, enabling lossless unification of detection and tracking tasks.  </p>
            </td>
            </tr>

        <!-- <tr>
            <td style="padding:20px;width:30%;max-width:30%" align="center">
                <img style="width:100%;max-width:100%" src="img/freevs.png" alt="dise">
            </td>
            <td width="75%" valign="center">
                <papertitle>Uncertain Object Representation for Image-Based 3D Object Perception</papertitle>
                <br>
                <strong>Qitai Wang</strong>, <a>Yuntao Chen</a>, <a>Zhaoxiang Zhang</a>
                <br>
                <em>TPAMI
                <br>
                <a href="">[paper]</a>
                <br>
                <p> We propose the uncertain representation of 3D objects to meet the indeterminacy of localizing objects in images  </p>
            </td>
            </tr> -->
      

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                <a href="https://jonbarron.info/">Website Template</a>
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
  
<p><center>
	  <!-- <div id="clustrmaps-widget" style="width:5%">
    <script type="text/javascript" id="clstr_globe" src="//clustrmaps.com/globe.js?d=pYUe-9vNee5nJl9ztd0lgo-xYiNaKRYwhjvT3xnX5Mg"></script>
	  </div>         -->
	  <br>
	    &copy; Qitai Wang | Last updated: May 05, 2025
</center></p>
</body>

</html>